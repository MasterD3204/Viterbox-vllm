{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misa-nlp/miniconda3/envs/misaTTS/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-07 10:31:49 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignored error while writing commit hash to /home/misa-nlp/.cache/soe_vinorm/models--vinhdq842--soe-vinorm/refs/cb9705b: [Errno 28] No space left on device.\n",
      "Fetching 3 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 20695.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß† STEP 1: Loading Base Viterbox Model...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giving vLLM 12.85% of GPU memory (3087.20 MB)\n",
      "INFO 01-07 10:31:53 [config.py:1604] Using max model len 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 10:31:53,183\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-07 10:31:53 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-07 10:31:53 [registry.py:430] Model architecture ChatterboxT3 is already registered, and will be overwritten by the new model class <class 'chatterbox_vllm.models.t3.t3.T3VllmModel'>.\n",
      "WARNING 01-07 10:31:53 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 01-07 10:31:56 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 01-07 10:31:56 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 01-07 10:31:56 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='./t3-model', speculative_config=None, tokenizer='ViTokenizer', skip_tokenizer_init=False, tokenizer_mode=custom, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=./t3-model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "INFO 01-07 10:31:58 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-07 10:31:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-07 10:31:58 [gpu_model_runner.py:1843] Starting to load model ./t3-model...\n",
      "INFO 01-07 10:31:58 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 01-07 10:31:58 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "Applying CFG scale: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 129.90it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-07 10:31:59 [default_loader.py:262] Loading weights took 0.38 seconds\n",
      "INFO 01-07 10:31:59 [gpu_model_runner.py:1892] Model loading took 1.0215 GiB and 0.498108 seconds\n",
      "INFO 01-07 10:31:59 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 178 conditionals items of the maximum feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-1 (_report_usage_worker):\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/misa-nlp/miniconda3/envs/misaTTS/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/misa-nlp/miniconda3/envs/misaTTS/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/misa-nlp/miniconda3/envs/misaTTS/lib/python3.10/site-packages/vllm/usage/usage_lib.py\", line 167, in _report_usage_worker\n",
      "    self._report_usage_once(model_architecture, usage_context, extra_kvs)\n",
      "  File \"/home/misa-nlp/miniconda3/envs/misaTTS/lib/python3.10/site-packages/vllm/usage/usage_lib.py\", line 223, in _report_usage_once\n",
      "    self._write_to_file(data)\n",
      "  File \"/home/misa-nlp/miniconda3/envs/misaTTS/lib/python3.10/site-packages/vllm/usage/usage_lib.py\", line 254, in _write_to_file\n",
      "    with open(_USAGE_STATS_JSON_PATH, \"a\") as f:\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-07 10:32:00 [gpu_worker.py:255] Available KV cache memory: 12.54 GiB\n",
      "INFO 01-07 10:32:00 [kv_cache_utils.py:833] GPU KV cache size: 109,568 tokens\n",
      "INFO 01-07 10:32:00 [kv_cache_utils.py:837] Maximum concurrency for 2,000 tokens per request: 54.78x\n",
      "INFO 01-07 10:32:00 [core.py:193] init engine (profile, create kv cache, warmup model) took 1.24 seconds\n",
      "WARNING 01-07 10:32:00 [registry.py:430] Model architecture ChatterboxT3 is already registered, and will be overwritten by the new model class <class 'chatterbox_vllm.models.t3.t3.T3VllmModel'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misa-nlp/miniconda3/envs/misaTTS/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base model loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # from src.chatterbox_vllm.models.t3.vitokenizer import ViTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from typing import List\n",
    "import torchaudio as ta\n",
    "from src.chatterbox_vllm.tts import ChatterboxTTS\n",
    "\n",
    "# ----- STEP 1: Load Base Model -----\n",
    "print(\"=\" * 60)\n",
    "print(\"üß† STEP 1: Loading Base Viterbox Model...\")\n",
    "print(\"=\" * 60)\n",
    "model = ChatterboxTTS.from_pretrained(\n",
    "    max_batch_size = 6,\n",
    "    max_model_len = 2000,\n",
    ")\n",
    "print(\"‚úÖ Base model loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebf146",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_prompt_path = \"/home/misa/Music/chatterbox-vllm/6.1.wav\"\n",
    "text = [\n",
    "    \"Tr√™n ƒë√¢y l√† to√†n b·ªô c√°c tin t·ª©c m·ªõi ƒë·ªÉ anh/ch·ªã ti·ªán theo d√µi. Xin c·∫£m ∆°n anh/ch·ªã ƒë√£ l·∫Øng nghe!\",\n",
    "    \"Xin ch√†o. Em c√≥ th·ªÉ xin h·ªç t√™n c·ªßa anh/ch·ªã ƒë∆∞·ª£c kh√¥ng ·∫°?\",\n",
    "    \"Xin l·ªói ch·ªã, em r·∫•t ti·∫øc v√¨ ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh l·∫•y d·ªØ li·ªáu. Em xin l·ªói v√¨ s·ª± b·∫•t ti·ªán n√†y, ch·ªã c√≥ mu·ªën th·ª≠ l·∫°i kh√¥ng, ho·∫∑c em c√≥ th·ªÉ h·ªó tr·ª£ v·∫•n ƒë·ªÅ g√¨ kh√°c cho ch·ªã ·∫°?\",\n",
    "    \"Ch√†o anh H√†, Em l√† tr·ª£ l√Ω s·ªë MISA AVA. Em c√≥ th·ªÉ gi√∫p g√¨ ƒë∆∞·ª£c cho anh?\",\n",
    "    \"Th∆∞a anh Th·∫Øng, h√¥m nay c√≥ 3 ng∆∞·ªùi sinh nh·∫≠t ·∫°: Nguy·ªÖn Th·ªã Hi·ªÅn - Nh√¢n vi√™n v·ªá sinh, ƒê·ªó Tu·∫•n Long - Ph√≥ Tr∆∞·ªüng ph√≤ng Kinh doanh, Nguy·ªÖn Th√†nh Lu√¢n - Nh√¢n vi√™n b·∫£o v·ªá. ƒê√¢y l√† l·ªùi ch√∫c sinh nh·∫≠t em ƒë√£ chu·∫©n b·ªã cho m·ªçi ng∆∞·ªùi v·ªõi c√πng m·ªôt n·ªôi dung\",\n",
    "    \"N·∫øu ch·ªã Th·ªßy c·∫ßn tra c·ª©u th√¥ng tin v·ªÅ nh√¢n s·ª± kh√°c ho·∫∑c c√≥ y√™u c·∫ßu c·ª• th·ªÉ h∆°n, ch·ªã vui l√≤ng cung c·∫•p th√™m th√¥ng tin ƒë·ªÉ em h·ªó tr·ª£ t·ªët h∆°n nh√©!\",\n",
    "    \"T√¨nh h√¨nh kinh doanh v√† t√†i ch√≠nh c·ªßa c√¥ng ty trong th√°ng 12 nƒÉm 2025 c√≥ s·ª± tƒÉng tr∆∞·ªüng t√≠ch c·ª±c, ƒë·∫∑c bi·ªát l√† doanh thu v√† l·ª£i nhu·∫≠n tr∆∞·ªõc thu·∫ø\",\n",
    "    \"Qu·ªëc h·ªôi quy·∫øt ƒë·ªãnh tƒÉng m·ª©c gi·∫£m tr·ª´ gia c·∫£nh t·ª´ nƒÉm 2026. Ng∆∞·ªùi n·ªôp thu·∫ø ƒë∆∞·ª£c gi·∫£m 15,5 tri·ªáu ƒë·ªìng/th√°ng, ng∆∞·ªùi ph·ª• thu·ªôc 6,2 tri·ªáu ƒë·ªìng/th√°ng. Ngh·ªã quy·∫øt c√≥ hi·ªáu l·ª±c t·ª´ 01/01/2026\",\n",
    "    \"N·∫øu c·∫ßn h·ªó tr·ª£ th√™m v·ªÅ th√¥ng tin b·ªánh vi·ªán ho·∫∑c quy tr√¨nh kh√°m ch·ªØa b·ªánh, anh ch·ªã c√≥ th·ªÉ li√™n h·ªá l·∫°i b·∫•t c·ª© l√∫c n√†o.\",\n",
    "    \"Ph·∫£i tr·∫£: Gi·∫£m 82,2% so v·ªõi c√πng k·ª≥ th√°ng 12/2024 t·ª´ -405,99 tri·ªáu ƒë·ªìng xu·ªëng -72,10 tri·ªáu ƒë·ªìng.\"\n",
    "]\n",
    "prompts = text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a618d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Reference mel length is not equal to 2 * reference token length.\n",
      "\n",
      "WARNING:root:Reference mel length is not equal to 2 * reference token length.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Processing 2 sentences in batch...\n",
      "  [1/2] Tr√™n ƒë√¢y l√† to√†n b·ªô c√°c tin t·ª©c m·ªõi ƒë·ªÉ anh/ch·ªã ti·ªá...\n",
      "  [2/2] Xin c·∫£m ∆°n anh/ch·ªã ƒë√£ l·∫Øng nghe ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 298.03it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.99it/s, est. speed input: 165.52 toks/s, output: 164.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T3] Batch generation: 1.02s for 2 sentences\n",
      "[S3Gen] Waveform generation: 0.83s\n"
     ]
    }
   ],
   "source": [
    "audio = model.generate(\n",
    "    text=prompts,\n",
    "    audio_prompt=audio_prompt_path,\n",
    "    language=\"vi\",\n",
    "    cfg_weight=0.7,      # TƒÉng l√™n 0.7 - 1.5 n·∫øu mu·ªën gi·ªçng b√°m s√°t LoRA h∆°n\n",
    "    temperature=0.1,     # Gi·∫£m temp ƒë·ªÉ gi·ªçng ·ªïn ƒë·ªãnh\n",
    "    split_sentences=True,\n",
    "    crossfade_ms = 80,\n",
    "    sentence_pause_ms = 100,\n",
    "    exaggeration = 2.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a670ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio shape: torch.Size([152256])\n",
      "Audio dtype: torch.float64\n",
      "Final audio shape: (145006,)\n",
      "‚úÖ Audio saved to: testcher.wav\n",
      "üìä Duration: 6.04 seconds\n",
      "üìä Sample rate: 24000 Hz\n",
      "\n",
      "üéâ DONE!\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "OUTPUT_FILE = \"test.wav\"\n",
    "\n",
    "# Ki·ªÉm tra shape c·ªßa audio\n",
    "print(f\"Audio shape: {audio[0].shape}\")\n",
    "print(f\"Audio dtype: {audio[0].dtype}\")\n",
    "\n",
    "# Convert to numpy\n",
    "audio_np = audio[0].cpu().numpy()\n",
    "\n",
    "# X·ª≠ l√Ω shape\n",
    "if audio_np.ndim == 1:\n",
    "    # ƒê√£ l√† 1D, OK\n",
    "    pass\n",
    "elif audio_np.ndim == 2:\n",
    "    if audio_np.shape[0] == 1:\n",
    "        # Shape (1, samples) -> (samples,)\n",
    "        audio_np = audio_np.squeeze(0)\n",
    "    elif audio_np.shape[1] == 1:\n",
    "        # Shape (samples, 1) -> (samples,)\n",
    "        audio_np = audio_np.squeeze(1)\n",
    "    elif audio_np.shape[0] < audio_np.shape[1]:\n",
    "        # Shape (channels, samples) -> (samples, channels)\n",
    "        audio_np = audio_np.T\n",
    "\n",
    "print(f\"Final audio shape: {audio_np.shape}\")\n",
    "\n",
    "# ƒê·∫£m b·∫£o dtype l√† float32 ho·∫∑c int16\n",
    "if audio_np.dtype not in [np.float32, np.float64, np.int16, np.int32]:\n",
    "    audio_np = audio_np.astype(np.float32)\n",
    "\n",
    "# Save file\n",
    "sf.write(OUTPUT_FILE, audio_np, 24000)\n",
    "\n",
    "print(f\"‚úÖ Audio saved to: {OUTPUT_FILE}\")\n",
    "print(f\"üìä Duration: {len(audio_np) / model.sr:.2f} seconds\")\n",
    "print(f\"üìä Sample rate: {model.sr} Hz\")\n",
    "print(\"\\nüéâ DONE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misaTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
